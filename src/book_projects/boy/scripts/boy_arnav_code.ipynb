{"cells":[{"cell_type":"markdown","metadata":{},"source":["Extracting chapters "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Chapter 1 extracted and saved as '../data/extracted_chapters/Chapter_1.txt'.\n","Chapter 2 extracted and saved as '../data/extracted_chapters/Chapter_2.txt'.\n","Chapter 3 extracted and saved as '../data/extracted_chapters/Chapter_3.txt'.\n","Chapter 4 extracted and saved as '../data/extracted_chapters/Chapter_4.txt'.\n","Chapter 5 extracted and saved as '../data/extracted_chapters/Chapter_5.txt'.\n","Chapter 6 extracted and saved as '../data/extracted_chapters/Chapter_6.txt'.\n","Chapter 7 extracted and saved as '../data/extracted_chapters/Chapter_7.txt'.\n","Chapter 8 extracted and saved as '../data/extracted_chapters/Chapter_8.txt'.\n","Chapter 9 extracted and saved as '../data/extracted_chapters/Chapter_9.txt'.\n","Chapter 10 extracted and saved as '../data/extracted_chapters/Chapter_10.txt'.\n","Chapter 11 extracted and saved as '../data/extracted_chapters/Chapter_11.txt'.\n","Chapter 12 extracted and saved as '../data/extracted_chapters/Chapter_12.txt'.\n","Chapter 13 extracted and saved as '../data/extracted_chapters/Chapter_13.txt'.\n","Chapter 14 extracted and saved as '../data/extracted_chapters/Chapter_14.txt'.\n","Chapter 15 extracted and saved as '../data/extracted_chapters/Chapter_15.txt'.\n","Chapter 16 extracted and saved as '../data/extracted_chapters/Chapter_16.txt'.\n","Chapter 17 extracted and saved as '../data/extracted_chapters/Chapter_17.txt'.\n","Chapter 18 extracted and saved as '../data/extracted_chapters/Chapter_18.txt'.\n","Chapter 19 extracted and saved as '../data/extracted_chapters/Chapter_19.txt'.\n","Chapter 20 extracted and saved as '../data/extracted_chapters/Chapter_20.txt'.\n","Chapter 21 extracted and saved as '../data/extracted_chapters/Chapter_21.txt'.\n","Chapter 22 extracted and saved as '../data/extracted_chapters/Chapter_22.txt'.\n","Chapter 23 extracted and saved as '../data/extracted_chapters/Chapter_23.txt'.\n","Chapter 24 extracted and saved as '../data/extracted_chapters/Chapter_24.txt'.\n","Chapter 25 extracted and saved as '../data/extracted_chapters/Chapter_25.txt'.\n"]}],"source":["# this is working but need to annotate. \n","import os\n","\n","def extract_chapters(book_file):\n","    with open(book_file, 'r', encoding='utf-8') as f:\n","        book_text = f.read()\n","\n","    chapters = book_text.split(\"CHAPTER\")\n","    chapters = chapters[1:]\n","\n","    # Adjusting the output directory to a relative path\n","    output_dir = \"../data/extracted_chapters\"\n","    if not os.path.exists(output_dir):\n","        os.mkdir(output_dir)\n","\n","    for i, chapter_text in enumerate(chapters, start=1):\n","        chapter_title = f\"Chapter_{i}\"\n","        chapter_filename = f\"{output_dir}/{chapter_title}.txt\"\n","\n","        with open(chapter_filename, 'w', encoding='utf-8') as chapter_file:\n","            chapter_file.write(chapter_text.strip())\n","\n","        print(f\"Chapter {i} extracted and saved as '{chapter_filename}'.\")\n","\n","# Adjusting the input file to a relative path\n","extract_chapters(\"../data/dahl-boy-4.txt\")\n"]},{"cell_type":"markdown","metadata":{},"source":["Running the booknlp pipeline\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Ah1lGVh5QQRv"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'pipeline': 'entity', 'model': 'big'}\n"]},{"ename":"HFValidationError","evalue":"Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'C:\\Users\\Raquel Coelho\\booknlps\\entities_google/bert_uncased_L-6_H-768_A-12'.","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Raquel Coelho\\Desktop\\Elalytics_Deepak\\elalytics-data-processing\\src\\book_projects\\boy\\scripts\\boy_arnav_code.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raquel%20Coelho/Desktop/Elalytics_Deepak/elalytics-data-processing/src/book_projects/boy/scripts/boy_arnav_code.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbooknlp\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbooknlp\u001b[39;00m \u001b[39mimport\u001b[39;00m BookNLP\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raquel%20Coelho/Desktop/Elalytics_Deepak/elalytics-data-processing/src/book_projects/boy/scripts/boy_arnav_code.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_params \u001b[39m=\u001b[39m {\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raquel%20Coelho/Desktop/Elalytics_Deepak/elalytics-data-processing/src/book_projects/boy/scripts/boy_arnav_code.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mpipeline\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mentity\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raquel%20Coelho/Desktop/Elalytics_Deepak/elalytics-data-processing/src/book_projects/boy/scripts/boy_arnav_code.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mbig\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Raquel%20Coelho/Desktop/Elalytics_Deepak/elalytics-data-processing/src/book_projects/boy/scripts/boy_arnav_code.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m }\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Raquel%20Coelho/Desktop/Elalytics_Deepak/elalytics-data-processing/src/book_projects/boy/scripts/boy_arnav_code.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m booknlp \u001b[39m=\u001b[39m BookNLP(\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m, model_params)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raquel%20Coelho/Desktop/Elalytics_Deepak/elalytics-data-processing/src/book_projects/boy/scripts/boy_arnav_code.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Relative path to the Chapter_1.txt file inside the data directory\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Raquel%20Coelho/Desktop/Elalytics_Deepak/elalytics-data-processing/src/book_projects/boy/scripts/boy_arnav_code.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m input_file \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../data/extracted_chapters/Chapter_1.txt\u001b[39m\u001b[39m\"\u001b[39m\n","File \u001b[1;32mc:\\Users\\Raquel Coelho\\Desktop\\Elalytics_Deepak\\elalytics-data-processing\\.venv\\Lib\\site-packages\\booknlp\\booknlp.py:14\u001b[0m, in \u001b[0;36mBookNLP.__init__\u001b[1;34m(self, language, model_params)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, language, model_params):\n\u001b[0;32m     13\u001b[0m \t\u001b[39mif\u001b[39;00m language \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m---> 14\u001b[0m \t\t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbooknlp\u001b[39m=\u001b[39mEnglishBookNLP(model_params)\n","File \u001b[1;32mc:\\Users\\Raquel Coelho\\Desktop\\Elalytics_Deepak\\elalytics-data-processing\\.venv\\Lib\\site-packages\\booknlp\\english\\english_booknlp.py:148\u001b[0m, in \u001b[0;36mEnglishBookNLP.__init__\u001b[1;34m(self, model_params)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquoteTagger\u001b[39m=\u001b[39mQuoteTagger()\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdoEntities:\n\u001b[1;32m--> 148\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentityTagger\u001b[39m=\u001b[39mLitBankEntityTagger(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mentityPath, tagsetPath)\n\u001b[0;32m    149\u001b[0m \taliasPath \u001b[39m=\u001b[39m pkg_resources\u001b[39m.\u001b[39mresource_filename(\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdata/aliases.txt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \t\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname_resolver\u001b[39m=\u001b[39mNameCoref(aliasPath)\n","File \u001b[1;32mc:\\Users\\Raquel Coelho\\Desktop\\Elalytics_Deepak\\elalytics-data-processing\\.venv\\Lib\\site-packages\\booknlp\\english\\entity_tagger.py:19\u001b[0m, in \u001b[0;36mLitBankEntityTagger.__init__\u001b[1;34m(self, model_file, model_tagset)\u001b[0m\n\u001b[0;32m     16\u001b[0m base_model\u001b[39m=\u001b[39mre\u001b[39m.\u001b[39msub(\u001b[39m\"\u001b[39m\u001b[39mgoogle_bert\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgoogle/bert\u001b[39m\u001b[39m\"\u001b[39m, model_file\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m     17\u001b[0m base_model\u001b[39m=\u001b[39mre\u001b[39m.\u001b[39msub(\u001b[39m\"\u001b[39m\u001b[39m.model\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, base_model)\n\u001b[1;32m---> 19\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m Tagger(freeze_bert\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, base_model\u001b[39m=\u001b[39;49mbase_model, tagset_flat\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mEVENT\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m1\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mO\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m1\u001b[39;49m}, supersense_tagset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msupersense_tagset, tagset\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtagset, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m     21\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(model_file, map_location\u001b[39m=\u001b[39mdevice))\n","File \u001b[1;32mc:\\Users\\Raquel Coelho\\Desktop\\Elalytics_Deepak\\elalytics-data-processing\\.venv\\Lib\\site-packages\\booknlp\\english\\tagger.py:58\u001b[0m, in \u001b[0;36mTagger.__init__\u001b[1;34m(self, freeze_bert, base_model, tagset, supersense_tagset, tagset_flat, hidden_dim, flat_hidden_dim, device)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrev_supersense_tagset[\u001b[39mlen\u001b[39m(supersense_tagset)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mO\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_labels_flat\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(tagset_flat)\n\u001b[1;32m---> 58\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(modelName, do_lower_case\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, do_basic_tokenize\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbert \u001b[39m=\u001b[39m BertModel\u001b[39m.\u001b[39mfrom_pretrained(modelName)\n\u001b[0;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39madd_tokens([\u001b[39m\"\u001b[39m\u001b[39m[CAP]\u001b[39m\u001b[39m\"\u001b[39m], special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n","File \u001b[1;32mc:\\Users\\Raquel Coelho\\Desktop\\Elalytics_Deepak\\elalytics-data-processing\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1784\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1782\u001b[0m             resolved_vocab_files[file_id] \u001b[39m=\u001b[39m download_url(file_path, proxies\u001b[39m=\u001b[39mproxies)\n\u001b[0;32m   1783\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1784\u001b[0m         resolved_vocab_files[file_id] \u001b[39m=\u001b[39m cached_file(\n\u001b[0;32m   1785\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   1786\u001b[0m             file_path,\n\u001b[0;32m   1787\u001b[0m             cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1788\u001b[0m             force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m   1789\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m   1790\u001b[0m             resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m   1791\u001b[0m             local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m   1792\u001b[0m             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1793\u001b[0m             user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m   1794\u001b[0m             revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   1795\u001b[0m             subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[0;32m   1796\u001b[0m             _raise_exceptions_for_missing_entries\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1797\u001b[0m             _raise_exceptions_for_connection_errors\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1798\u001b[0m             _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[0;32m   1799\u001b[0m         )\n\u001b[0;32m   1800\u001b[0m         commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_vocab_files[file_id], commit_hash)\n\u001b[0;32m   1802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(unresolved_files) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n","File \u001b[1;32mc:\\Users\\Raquel Coelho\\Desktop\\Elalytics_Deepak\\elalytics-data-processing\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    414\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[0;32m    418\u001b[0m         path_or_repo_id,\n\u001b[0;32m    419\u001b[0m         filename,\n\u001b[0;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[0;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[0;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[0;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[0;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[0;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[0;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[0;32m    430\u001b[0m     )\n\u001b[0;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m    433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m     )\n","File \u001b[1;32mc:\\Users\\Raquel Coelho\\Desktop\\Elalytics_Deepak\\elalytics-data-processing\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m arg_name, arg_value \u001b[39min\u001b[39;00m chain(\n\u001b[0;32m    106\u001b[0m     \u001b[39mzip\u001b[39m(signature\u001b[39m.\u001b[39mparameters, args),  \u001b[39m# Args values\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     kwargs\u001b[39m.\u001b[39mitems(),  \u001b[39m# Kwargs values\u001b[39;00m\n\u001b[0;32m    108\u001b[0m ):\n\u001b[0;32m    109\u001b[0m     \u001b[39mif\u001b[39;00m arg_name \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfrom_id\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mto_id\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m--> 110\u001b[0m         validate_repo_id(arg_value)\n\u001b[0;32m    112\u001b[0m     \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m         has_token \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Raquel Coelho\\Desktop\\Elalytics_Deepak\\elalytics-data-processing\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:164\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    159\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 164\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    165\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must use alphanumeric chars or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m--\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m\u001b[39m are\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    166\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m forbidden, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m and \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m cannot start or end the name, max length is 96:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    167\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m--\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m repo_id:\n\u001b[0;32m    171\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot have -- or .. in repo_id: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'C:\\Users\\Raquel Coelho\\booknlps\\entities_google/bert_uncased_L-6_H-768_A-12'."]}],"source":["# this is not working for me \n","\n","from booknlp.booknlp import BookNLP\n","\n","model_params = {\n","    \"pipeline\": \"entity\",\n","    \"model\": \"big\"\n","}\n","\n","booknlp = BookNLP(\"en\", model_params)\n","\n","# Relative path to the Chapter_1.txt file inside the data directory\n","input_file = \"../data/extracted_chapters/Chapter_1.txt\"\n","\n","# Relative path to the boy_results directory\n","# Assuming you want to create this directory at the same level as data and scripts\n","output_dir = \"../boy_results\"\n","\n","book_id = \"dahl_boy2\"\n","\n","booknlp.process(input_file, output_dir, book_id)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7T9giE4lY9GV"},"source":["Analyze files\n","BTW, you can use markdown notation in these cells! Add # for first heading, ## for second heading, etc. It will also allow you to collapse cells under a heading.\n","\n","#My analyses\n"]},{"cell_type":"markdown","metadata":{"id":"ie__h2YVoS6Z"},"source":["#Emotion Detection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_XgEP2zoUkD"},"outputs":[],"source":["# Raquel - I have not look at this yet\n","!pip install transformers\n","from transformers import pipeline\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import json\n","\n","quotes_file = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/dahl_boy.quotes\"\n","\n","def get_descriptors(file):\n","  main_ID = {472:\"Mother\", 0:\"Boy\", 679:\"Headmaster\", 172:\"Coombes\", 174:\"Hardcastle\"}\n","  classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n","\n","  quotes = pd.read_csv(file, sep = '\\t')\n","\n","  emotions = {\"Mother\":{\"anger\": 0, \"disgust\":0, \"fear\":0, \"joy\":0, \"sadness\":0, \"surprise\":0}\n","              ,\"Boy\":{\"anger\": 0, \"disgust\":0, \"fear\":0, \"joy\":0, \"sadness\":0, \"surprise\":0}\n","              ,\"Headmaster\":{\"anger\": 0, \"disgust\":0, \"fear\":0, \"joy\":0, \"sadness\":0, \"surprise\":0},\n","              \"Coombes\":{\"anger\": 0, \"disgust\":0, \"fear\":0, \"joy\":0, \"sadness\":0, \"surprise\":0},\n","              \"Hardcastle\":{\"anger\": 0, \"disgust\":0, \"fear\":0, \"joy\":0, \"sadness\":0, \"surprise\":0}}\n","\n","\n","\n","  for index, row in quotes.iterrows():\n","    if row[\"char_id\"] in main_ID:\n","      name = main_ID[row[\"char_id\"]]\n","      temp = classifier(row[\"quote\"])\n","      for i in range(7):\n","        if i == 4:\n","          continue\n","        emotion = temp[0][i][\"label\"]\n","        value = temp[0][i][\"score\"]\n","        if value > .3:\n","          emotions[name][emotion] += 1\n","\n","  data = json.dumps(emotions)\n","  json_file = \"/content/drive/Shareddrives/ELALytics/Visualizations/Boy/Data/emotions_by_chararcter.json\"\n","  with open(json_file, 'w') as file:\n","    file.write(data)\n","\n","\n","\n","      # anger.append(temp[0][0][\"score\"])\n","      # disgust.append(temp[0][1][\"score\"])\n","      # fear.append(temp[0][2][\"score\"])\n","      # joy.append(temp[0][3][\"score\"])\n","      # neutral.append(temp[0][4][\"score\"])\n","      # sadness.append(temp[0][5][\"score\"])\n","      # surprise.append(temp[0][6][\"score\"])\n","\n","  # # plt.hist(anger)\n","  # plt.hist(sadness)\n","  # # plt.hist(neutral)\n","  # # plt.hist(joy)\n","\n","    #use .4 as the marker\n","get_descriptors(quotes_file)\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FRCAdM6ZTOoi"},"source":["#Sentiment Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2vrluLDTQiU"},"outputs":[],"source":["import spacy\n","!pip install textblob\n","\n","from textblob import TextBlob\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","dir = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/extracted_chapters/\"\n","\n","def sentences(input):\n","  sentiment_scores = []\n","  with open(input, \"r\", encoding=\"utf-8\") as f:\n","    text = f.read()\n","\n","  doc = nlp(text)\n","  for sent in doc.sents:\n","    sentiment = TextBlob(sent.text).sentiment.polarity\n","    sentiment_scores.append(sentiment)\n","\n","  average = sum(sentiment_scores) / len(sentiment_scores)\n","  return average\n","\n","i = 1\n","dict = {}\n","for file in os.listdir(dir):\n","  f = os.path.join(dir, file)\n","  avg = sentences(f)\n","  temp = \"Chapter_\" + str(i)\n","  dict[temp] = avg\n","  print(f\"Chapter {i} done\")\n","  i += 1\n","\n","data = json.dumps(dict)\n","json_file = \"/content/drive/Shareddrives/ELALytics/Visualizations/Boy/Data/chapter_sentiment.json\"\n","with open(json_file, 'w') as file:\n","  file.write(data)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bt0TQXry2JYp"},"source":["#Conflict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aYarqYSG2OVr"},"outputs":[],"source":["import pandas as pd\n","import spacy\n","import json\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","lexicon_df = pd.read_csv(\"/content/drive/MyDrive/Colab_Notebooks/expanding_ela/boy_results/NRC-VAD-Lexicon.csv\", sep = \"\\t\")\n","\n","\n","text = \"\"\n","\n","with open(\"/content/drive/MyDrive/Colab_Notebooks/Copy of 0. The Short Story of _The Lottery_ by_ Shirley Jackson.txt\", \"r\") as file:\n","  text = file.read()\n","\n","paragraphs = []\n","\n","for i in range(1, 82):\n","  look = f\"P{i}\"\n","  look2 = f\"P{i + 1}\"\n","  start = text.find(look)\n","  end = text.find(look2)\n","  if i == 81:\n","    para = text[start + 3:]\n","  else:\n","    if i > 9:\n","      para = text[start + 3:end]\n","    else:\n","      para = text[start+2:end]\n","  paragraphs.append(para)\n","\n","for para in paragraphs:\n","  print(para)\n","\n","\n","def get_negative_valence_arousal_score(sentence, lexicon_df):\n","    # Tokenize sentence into words\n","    doc = nlp(sentence)\n","\n","    # Look up valence and arousal scores for each word in the lexicon\n","    scores = []\n","\n","    for token in doc:\n","      word = token.text\n","      if not word.isalnum():\n","        continue\n","      word_data = lexicon_df.loc[lexicon_df['Word'] == word]\n","      if not word_data.empty:\n","          valence = word_data['Valence'].values[0]\n","          arousal = word_data['Arousal'].values[0]\n","          # Compute negative valence (unpleasantness) by subtracting valence score from 1\n","          negative_valence = 1 - valence\n","          # Compute negative valence + arousal\n","          scores.append(negative_valence + arousal)\n","\n","  # Calculate average score at the sentence level\n","    if scores:\n","        return sum(scores) / len(scores)\n","    else:\n","        return None\n","\n","para_scores = {}\n","for index, para in enumerate(paragraphs):\n","  sentence_scores = []\n","  doc = nlp(para)\n","  max_sent = \"\"\n","  max_score = -100\n","  for sent in doc.sents:\n","    score = get_negative_valence_arousal_score(sent.text, lexicon_df)\n","    if score and score > max_score:\n","      max = score\n","      max_sent = sent.text\n","    sentence_scores.append(score)\n","  sentence_scores = [score for score in sentence_scores if score is not None]\n","  if not sentence_scores:\n","    print(para)\n","    continue\n","  average_score = sum(sentence_scores) / len(sentence_scores)\n","  para_scores[index + 1] = [average_score , max_sent]\n","\n","data = json.dumps(para_scores)\n","\n","with open(\"/content/drive/Shareddrives/ELALytics/Visualizations/Boy/Data/letter_conflict_with_sentences.json\", \"w\") as json_file:\n","  json_file.write(data)\n"]},{"cell_type":"markdown","metadata":{"id":"w9VCfGsIOqqo"},"source":["#word cloud"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8347,"status":"ok","timestamp":1690936348487,"user":{"displayName":"Arnav Gupta","userId":"02401813659526018071"},"user_tz":420},"id":"c7AYd6uvMdjP","outputId":"2f1bf386-4cbf-4e29-d98a-b02fef5ab6d9"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#THIS IS THE DATA FOR WORD CLOUD. Extracts most common lemmas\n","#doesn't filter out filler words / articles like 'and' or 'uh', in\n","#future I might add a filter to this\n","\n","# !pip install stop_words\n","# !pip install nltk\n","\n","# !pip install booknlp\n","\n","# from booknlp.booknlp import BookNLP\n","# import os\n","\n","\n","# #sets the pipeline, we want to do a full pipeline (usually)\n","# model_params={\n","#                 \"pipeline\":\"entity,event,coref,quote,supersense\",\n","#                 \"model\":\"big\"\n","\n","#                 }\n","\n","# booknlp=BookNLP(\"en\", model_params)\n","\n","\n","input_dir = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/extracted_chapters/\"\n","output_dir = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/extracted_chapters/\"\n","\n","####################\n","\n","import nltk\n","import json\n","import pandas as pd\n","\n","punc = [\"!\", \" \", \".\", \",\", \"?\", \"-\", \"|\", \"'s\", \";\", \"'\", \"...\", \"(\", \")\"]\n","\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from google.colab import drive\n","drive.mount('/content/drive')\n","from google.colab import files\n","\n","# token_file = 'drive/MyDrive/Colab_Notebooks/boy_results/tokens.tsv'\n","# path = 'drive/MyDrive/Colab_Notebooks/boy_results/'\n","\n","# output_name = \"topwordsPOS\"\n","def get_tokens(token_file, path, name):\n","\n","  token_df = pd.read_csv(token_file, sep = '\\t')\n","  lemmas_POS = token_df[[\"lemma\",\"POS_tag\"]]\n","  words = lemmas_POS[\"lemma\"]\n","\n","  def filter(word):\n","    if word not in stopwords.words('english') and not word in punc:\n","      return True\n","    return False\n","\n","  filtered = words[words.apply(filter)]\n","  counts = filtered.value_counts().reset_index()\n","  counts = counts[:100]\n","  counts.columns = [\"lemma\", \"Counts\"]\n","  lemmas_POS = pd.merge(lemmas_POS, counts, on=\"lemma\", how = \"left\")\n","  lemmas_POS = lemmas_POS.dropna(subset=\"Counts\")\n","  lemmas_POS.drop_duplicates(subset=[\"lemma\"],inplace=True)\n","  lemmas_POS = lemmas_POS.sort_values(by=\"Counts\", ascending = False)\n","  lemmas_POS.rename(columns={\"lemma\":\"word\", \"POS_tag\":\"category\", \"Counts\":\"value\"}, inplace=True)\n","  lemmas_POS.to_csv(path + name + \".tsv\", sep = '\\t')\n","  dict = lemmas_POS.to_dict(orient='records')\n","  data = json.dumps(dict)\n","  json_file = path + name + \".json\"\n","  with open(json_file, 'w') as file:\n","    file.write(data)\n","\n","# get_tokens(token_file)\n","\n","i = 1\n","for file in os.listdir(input_dir):\n","  if not file.endswith(\".tokens\"):\n","    continue\n","  f = os.path.join(input_dir, file)\n","  name = \"boy_topwords_chapter_\" + str(i)\n","  get_tokens(f, output_dir, name)\n","  i += 1\n","  # book_id = \"dahl_boy_chapter_\" + str(i)\n","  # booknlp.process(f, output_dir, book_id)\n","  # i += 1\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"451Z3dqaCtSb"},"source":["#Distinctive Words\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"elapsed":2622,"status":"error","timestamp":1694798451168,"user":{"displayName":"Deepak Varuvel Dennison","userId":"08613606674423185455"},"user_tz":420},"id":"-_mdWksU2-G6","outputId":"f08129cd-234b-4b3b-c671-dd4e208c3137"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-34e83486b3cc>\u001b[0m in \u001b[0;36m<cell line: 50>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"category\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"value\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab_Notebooks/boy_results/extracted_chapters'"]}],"source":["#create words df\n","\n","import pandas as pd\n","import os\n","import nltk\n","punc = [\"!\", \" \", \".\", \",\", \"_\", \"?\", \"-\", \"|\", \"'s\", \";\", \"'\", \"...\", \"(\", \")\"]\n","\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","\n","chapter_names = [\"Papa and Mama\", \"Kindergarten, 1922-3 (age 6-7)\", \"The bicycle and the sweet-shop\",\n","                 \"The Great Mouse Plot\", \"Mr Coombes\", \"Mrs Pratchett's Revenge\", \"Going to Norway\",\n","                 \"The Magic Island\", \"A Visit to the Doctor\", \"First Day\", \"Writing Home\", \"The Matron\",\n","                 \"Homesickness\", \"A Drive in the Motor-car\", \"Captain Hardcastle\", \"Little Ellis and the Boil\",\n","                 \"Goat's Tobacco\", \"Getting Dressed for the Big School\", \"Boazers\", \"The Headmaster\",\n","                 \"Chocolates\",\"Corkers\", \"Being Bossed Around\", \"Games and Photography\",\n","                 \"Goodbye School\"]\n","\n","input_dir = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/extracted_chapters\"\n","\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","\n","def get_tokens(token_file, chapter):\n","\n","  token_df = pd.read_csv(token_file, sep = '\\t')\n","  lemmas_POS = token_df[[\"lemma\",\"POS_tag\"]]\n","  words = lemmas_POS[\"lemma\"]\n","\n","  def filter(word):\n","    if word not in stopwords.words('english') and not word in punc:\n","      return True\n","    return False\n","\n","  filtered = words[words.apply(filter)]\n","  counts = filtered.value_counts().reset_index()\n","  counts.columns = [\"lemma\", \"Counts\"]\n","  lemmas_POS = pd.merge(lemmas_POS, counts, on=\"lemma\", how = \"left\")\n","  lemmas_POS = lemmas_POS.dropna(subset=\"Counts\")\n","  lemmas_POS.drop_duplicates(subset=[\"lemma\"],inplace=True)\n","  lemmas_POS = lemmas_POS.sort_values(by=\"Counts\", ascending = False)\n","  lemmas_POS[\"chapter\"] = chapter\n","  lemmas_POS.rename(columns={\"lemma\":\"word\", \"POS_tag\":\"category\", \"Counts\":\"value\"}, inplace=True)\n","\n","  return lemmas_POS\n","\n","result = pd.DataFrame(columns=[\"word\", \"category\", \"value\"])\n","i = 1\n","for file in os.listdir(input_dir):\n","  if not file.endswith(\".tokens\"):\n","      continue\n","  else:\n","    file = input_dir + \"/\" + file\n","    temp = get_tokens(file, i)\n","  result = pd.concat([result, temp], ignore_index = True)\n","  i += 1\n","\n","word_df = result\n","\n","# # Define dictionary to map original POS tags to EVEN MORE simplified tags\n","pos_dict_most_simplified = {'NOUN': 'noun',\n","            'PROPN': 'noun',\n","                            \"PRON\":\"noun\",\n","\n","            'VERB': 'verb',\n","            'ADJ': 'adj',\n","            'ADV': 'adv',\n"," }\n","\n","# Define function to simplify POS tags\n","def most_simplify_pos_tag(tag):\n","    return pos_dict_most_simplified.get(tag, 'other')\n","\n","word_df['pos_more_simple'] = word_df['category'].apply(most_simplify_pos_tag)\n","\n","\n","\n","# result.to_csv(\"/content/drive/MyDrive/Colab_Notebooks/boy_results/boy_words.tsv\", sep = '\\t')\n","\n","from collections import defaultdict\n","import math\n","import operator\n","\n","\n","def log_odds(counts1, counts2, prior, zscore = True):\n","    # code from Dan Jurafsky\n","    # note: counts1 will be positive and counts2 will be negative\n","\n","    sigmasquared = defaultdict(float)\n","    sigma = defaultdict(float)\n","    delta = defaultdict(float)\n","\n","    n1 = sum(counts1.values())\n","    n2 = sum(counts2.values())\n","\n","    # since we use the sum of counts from the two groups as a prior, this is equivalent to a simple log odds ratio\n","    nprior = sum(prior.values())\n","    for word in prior.keys():\n","        if prior[word] == 0:\n","            delta[word] = 0\n","            continue\n","        if word not in counts1:\n","            counts1[word] = 0\n","        if word not in counts2:\n","            counts2[word] = 0\n","        l1 = float(counts1[word] + prior[word]) / (( n1 + nprior ) - (counts1[word] + prior[word]))\n","        l2 = float(counts2[word] + prior[word]) / (( n2 + nprior ) - (counts2[word] + prior[word]))\n","        sigmasquared[word] = 1/(float(counts1[word]) + float(prior[word])) + 1/(float(counts2[word]) + float(prior[word]))\n","        sigma[word] = math.sqrt(sigmasquared[word])\n","        delta[word] = (math.log(l1) - math.log(l2))\n","        if zscore:\n","            delta[word] /= sigma[word]\n","    return delta\n","\n","prior_counts = word_df.groupby(\"word\")['value'].sum().to_dict()\n","\n","from matplotlib.colors import ListedColormap\n","# Define a color function that maps POS tags to colors\n","# simple_pos_levels = word_df['pos_more_simple'].unique()\n","# num_pos_levels = len(simple_pos_levels)\n","# colors = ListedColormap(plt.cm.Set1(range(num_pos_levels)))\n","# pos_color_dict = dict(zip(simple_pos_levels, colors(range(num_pos_levels))))\n","# def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n","#     return pos_color_dict[word_df.loc[word_df['word'] == word, 'pos_more_simple'].values[0]]\n","\n","\n","pos_color_dict = {'noun': (31, 119, 180),  # blue\n","                  'adj': (242, 223, 58),  # yellow\n","                  'adv': (0, 102, 0),  # green\n","                  'verb': (245, 139, 213)}  # pink\n","default_color = (128, 128, 128)  # grey\n","def color_func2(word, font_size, position, orientation, random_state=None, **kwargs):\n","    return pos_color_dict.get(word_df.loc[word_df['word'] == word, 'pos_more_simple'].values[0], default_color)\n","\n","for i in range(1, 26):\n","  print(\"CHAPTER \" + str(i))\n","  chap_counts = word_df[word_df['chapter'] == i].set_index('word')['value'].to_dict()\n","  other_counts = word_df[word_df['chapter'] != i].groupby(\"word\")['value'].sum().to_dict()\n","  delta = log_odds(chap_counts, other_counts, prior_counts, True)\n","  delta_sorted = sorted(delta.items(), key=operator.itemgetter(1))\n","  chap_words = []\n","  for k, v in reversed(delta_sorted):\n","      if v >= 1.5:\n","          chap_words.append(\"%s (%.3f)\" % (k, v))\n","  to_plot = {k: v for k, v in reversed(delta_sorted) if v > 0}\n","  wordcloud = WordCloud(width=500, height=500, background_color='white',\n","                         max_words=100, color_func=color_func2,\n","                        min_font_size = 15).generate_from_frequencies(to_plot)\n","  plt.figure(figsize=(5, 5), dpi=800, facecolor=None)\n","  plt.title(chapter_names[i-1], size=20)\n","  plt.imshow(wordcloud)\n","  plt.axis(\"off\")\n","  plt.tight_layout(pad=0)\n","  temp = \"CHAPTER\" + str(i) + \".png\"\n","  plt.savefig(input_dir + \"/\" +  temp)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4WTD04pCyOD"},"outputs":[],"source":["from collections import defaultdict\n","import math\n","\n","\n","def log_odds(counts1, counts2, prior, zscore = True):\n","    # code from Dan Jurafsky\n","    # note: counts1 will be positive and counts2 will be negative\n","\n","    sigmasquared = defaultdict(float)\n","    sigma = defaultdict(float)\n","    delta = defaultdict(float)\n","\n","    n1 = sum(counts1.values())\n","    n2 = sum(counts2.values())\n","\n","    # since we use the sum of counts from the two groups as a prior, this is equivalent to a simple log odds ratio\n","    nprior = sum(prior.values())\n","    for word in prior.keys():\n","        if prior[word] == 0:\n","            delta[word] = 0\n","            continue\n","        if word not in counts1:\n","            counts1[word] = 0\n","        if word not in counts2:\n","            counts2[word] = 0\n","        l1 = float(counts1[word] + prior[word]) / (( n1 + nprior ) - (counts1[word] + prior[word]))\n","        l2 = float(counts2[word] + prior[word]) / (( n2 + nprior ) - (counts2[word] + prior[word]))\n","        sigmasquared[word] = 1/(float(counts1[word]) + float(prior[word])) + 1/(float(counts2[word]) + float(prior[word]))\n","        sigma[word] = math.sqrt(sigmasquared[word])\n","        delta[word] = (math.log(l1) - math.log(l2))\n","        if zscore:\n","            delta[word] /= sigma[word]\n","    return delta\n","\n","\n","prior_counts = word_df.groupby(\"word\")['count'].sum().to_dict()\n","\n","for i in chapters:\n","    print(\"CHAPTER %d: %s\" % (i, chapter_names[i]))\n","    chap_counts = word_df[word_df['chapter'] == i].set_index('word')['count'].to_dict()\n","    other_counts = word_df[word_df['chapter'] != i].groupby(\"word\")['count'].sum().to_dict()\n","    delta = log_odds(chap_counts, other_counts, prior_counts, True)\n","    delta_sorted = sorted(delta.items(), key=operator.itemgetter(1))\n","    chap_words = []\n","    for k, v in reversed(delta_sorted):\n","        if v >= 1.5:\n","            chap_words.append(\"%s (%.3f)\" % (k, v))\n","    print(\"\\n\".join(chap_words))\n","    to_plot = {k: v for k, v in reversed(delta_sorted) if v > 0}\n","    print(to_plot)\n","    print('-------')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YZH5QQkPS9XO"},"source":["#Descriptors by Character  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYOI2D2SS8cM"},"outputs":[],"source":["import json\n","import pandas as pd\n","import csv\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","book_file = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/dahl_boy.book\"\n","quotes_file = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/dahl_boy.quotes\"\n","\n","#Copied from Example Colab\n","# def get_NRC_lexicon():\n","#     '''\n","#     @output:\n","#     - A chars[name][character]ionary of format {word : score}\n","#     '''\n","#     lexicon = '/content/drive/MyDrive/Colab_Notebooks/boy_results/NRC-VAD-Lexicon.txt'\n","#     val_chars[character] = {}\n","#     aro_chars[character] = {}\n","#     dom_chars[character] = {}\n","#     with open(lexicon, 'r') as infile:\n","#         reader = csv.chars[name][character]Reader(infile, delimiter=\"\\t\")\n","#         for row in reader:\n","#             word = row['Word']\n","#             val_chars[character][word] = float(row['Valence'])\n","#             aro_chars[character][word] = float(row['Arousal'])\n","#             dom_chars[character][word] = float(row['Dominance'])\n","#     return (val_chars[character], aro_chars[character], dom_chars[character])\n","\n","# # save VAD chars[name][character]ionaries\n","# val_chars[character], aro_chars[character], dom_chars[character] = get_NRC_lexicon()\n","\n","\n","#extract descriptors by gender + run VAD\n","\n","def get_descriptors(file):\n","  main_ID = {472:\"Mother\", 0:\"Boy\", 679:\"Headmaster\", 172:\"Coombes\", 174:\"Hardcastle\"}\n","\n","  quotes = pd.read_csv(file, sep = '\\t')\n","\n","  for index, row in quotes.iterrows():\n","    if row[\"char_id\"] in main_ID:\n","\n","\n","\n","  # with open(file, \"r\") as file:\n","  #   book_data = json.load(file)\n","\n","\n","    # if character[\"mentions\"][\"proper\"]:\n","    #   if character[\"mentions\"][\"proper\"][0]['n'] not in main:\n","    #     continue\n","    #   else:\n","    #     name = character[\"mentions\"][\"proper\"][0][\"n\"]\n","    #     ID = character[\"id\"]\n","    # elif character[\"mentions\"][\"common\"]:\n","    #   if character[\"mentions\"][\"common\"][0][\"n\"] not in main:\n","    #     continue\n","    #   else:\n","    #     name = character[\"mentions\"][\"common\"][0][\"n\"]\n","    #     ID = character[\"id\"]\n","    # else:\n","    #   if character[\"mentions\"][\"pronoun\"] and character[\"mentions\"][\"pronoun\"][0][\"n\"] in main:\n","    #     name = \"I\"\n","    #   else:\n","    #     continue\n","    # chars[name] = {\"poss\":{}, \"agents\":{}, \"pats\":{}}\n","  #   # else:\n","  #   #   name = character[\"mentions\"][\"common\"][0]['n']\n","  #   # chars[name][character] = {\"adjs\":[], \"agents\":[], \"pats\":[]}\n","  #   # if character[\"g\"] is None:\n","  #   #   continue\n","  #   #take character attributes\n","  #   for poss in character[\"poss\"]:\n","  #       # possL.append(poss['w'])\n","  #     if not poss['w'] in chars[name][\"poss\"]:\n","  #       chars[name][\"poss\"][poss['w']] = 0\n","  #     chars[name][\"poss\"][poss['w']] += 1\n","  #   # for patient in character[\"patient\"]:\n","  #   #     # patsL.append(patient['w'])\n","  #   #   if not patient['w'] in chars[name][\"pats\"]:\n","  #   #     chars[name][\"pats\"][patient['w']] = 0\n","  #   #   chars[name][\"pats\"][patient['w']] += 1\n","  #   for agent in character[\"agent\"]:\n","  #     if not agent['w'] in chars[name][\"agents\"]:\n","  #       chars[name][\"agents\"][agent['w']] = 0\n","  #     chars[name][\"agents\"][agent['w']] += 1\n","  #       # agentL.append(agent['w'])\n","  #   result[name] =  {\"poss\":{}, \"agents\":{}, \"pats\":{}}\n","  #   for key, val in sorted(chars[name][\"agents\"].items(), key= lambda tuple: tuple[1] , reverse=True)[:15]:\n","  #     result[name][\"agents\"][key] = val\n","  #   for key, val in sorted(chars[name][\"poss\"].items(), key= lambda tuple: tuple[1], reverse=True)[:15]:\n","  #     result[name][\"poss\"][key] = val\n","  #   for key, val in sorted(chars[name][\"pats\"].items(), key = lambda tuple: tuple[1], reverse=True)[:15]:\n","  #     result[name][\"pats\"][key] = val\n","  # print(result)\n","\n","\n","  # list = []\n","  # for character in result:\n","  #   words_list = []\n","  #   for word, count in result[character][\"poss\"].items():\n","  #     words_list.append((word,count,\"poss\"))\n","  #   for word, count in result[character][\"agents\"].items():\n","  #     words_list.append((word, count, \"agent\"))\n","  #   # for word, count in result[character][\"pats\"].items():\n","  #   #   words_list.append((word, count, \"patient\"))\n","  #   words_list = sorted(words_list, key = lambda x: x[1], reverse=True)\n","  #   i = 1\n","  #   for word in words_list[:15]:\n","  #     if character == \"my mother\":\n","  #       gender = \"female\"\n","  #     else:\n","  #       gender = \"male\"\n","  #     list.append({\"word\":word[0], \"character\":character, \"rank\":str(i), \"count\":word[1], \"type\":word[2], \"gender\":gender})\n","  #     i += 1\n","\n","  # data = json.dumps(list)\n","  # json_file = \"/content/drive/Shareddrives/ELALytics/Visualizations/Boy/Data/character_words_NO_PATIENT.json\"\n","  # with open(json_file, 'w') as file:\n","  #   file.write(data)\n","\n","\n","  # poss_series = pd.Series(possL)\n","  # pats_series = pd.Series(patsL)\n","  # agent_series = pd.Series(agentL)\n","\n","  # poss_series = poss_series.value_counts()[:15]\n","  # agent_series = agent_series.value_counts()[:15]\n","  # pats_series = pats_series.value_counts()[:15]\n","\n","  # print(poss_series.index)\n","  # print(poss_series.values)\n","\n","  # results = pd.DataFrame({\"possessive\":poss_series.index, \"poss_count\":poss_series.values,\n","  #         \"agents\":agent_series.index, \"age_count\":agent_series.values ,\n","  #         \"patient\":pats_series.index, \"pat_count\":pats_series.values})\n","\n","  # results.to_csv(\"/content/drive/Shareddrives/ELALytics/Visualizations/Boy/Data/poss_agent_patient_list.tsv\",\n","  #                sep = '\\t')\n","  # dict = results.to_dict(orient=\"records\")\n","\n","\n","\n","\n","\n","\n","  # print(\"POSSESSIVE VERBS\")\n","  # for poss in sorted(chars[name][\"poss\"],key = lambda word: chars[name][\"poss\"].get(word), reverse=True)[:10]:\n","  #   print(poss, chars[name][\"poss\"][poss])\n","  #   results[\"\"]\n","  # print(\"\\n\")\n","  # print(\"PATIENT VERBS\")\n","  # for pat in sorted(chars[name][\"pats\"],key = lambda word: chars[name][\"pats\"].get(word), reverse=True)[:10]:\n","  #   print(pat, chars[name][\"pats\"][pat])\n","  # print(\"\\n\")\n","  # print(\"AGENT VERBS\")\n","  # for agent in sorted(chars[name][\"agents\"],key = lambda word: chars[name][\"agents\"].get(word), reverse=True)[:10]:\n","  #   print(agent, chars[name][\"agents\"][agent])\n","\n","    # for name in character[\"mentions\"][\"common\"][:10]:\n","    #   chars[name][character][\"names\"].append(name['n'])\n","  # male = pd.Series(male_adjs)\n","  # female = pd.Series(female_adjs)\n","  # male = male.drop_duplicates()\n","  # female = female.drop_duplicates()\n","  # df = pd.DataFrame()\n","  # df[\"Male Descriptors\"] = male\n","  # df[\"Female Descriptors\"] = female\n","  #m_val, m_arous, m_dom, f_val, f_arous, f_dom = ([] for i in range(6))\n","  #Get VAD scores for adjectives\n","  # for adj in female_adjs:\n","  #   if adj in list(val_chars[character].keys()):\n","  #     f_val.append(val_chars[character][adj])\n","  #   else:\n","  #     f_val.append(999)\n","  #   # if adj in list(aro_chars[character].keys()):\n","  #   #   f_arous.append(aro_chars[character][adj])\n","  #   # else:\n","  #   #   f_arous.append(999)\n","  #   # if adj in list(dom_chars[character].keys()):\n","  #   #   f_dom.append(dom_chars[character][adj])\n","  #   # else:\n","  #   #   f_dom.append(999)\n","\n","  # for adj in male_adjs:\n","  #   if adj in list(val_chars[character].keys()):\n","  #     m_val.append(val_chars[character][adj])\n","  #   else:\n","  #     m_val.append(999)\n","    # if adj in list(aro_chars[character].keys()):\n","    #   m_arous.append(aro_chars[character][adj])\n","    # else:\n","    #   m_arous.append(999)\n","    # if adj in list(dom_chars[character].keys()):\n","    #   m_dom.append(dom_chars[character][adj])\n","    # else:\n","    #   m_dom.append(999)\n","\n","  # df.insert(1, \"MValence\", m_val)\n","  #df.insert(2, \"MArousal\", m_arous)\n","  #df.insert(3, \"MDominance\", m_dom)\n","  # df.insert(5, \"FValence\", f_val)\n","  #df.insert(6, \"FArousal\", f_arous)\n","  #df.insert(7, \"FDominance\", f_dom)\n","\n","  # df = pd.DataFrame()\n","  # var = [male, female]\n","  # for gender in var:\n","  #   chars[name][character]2 = {}\n","  #   for key, value2 in gender.items():\n","  #     series = pd.Series(data=value2, name = key)\n","  #     series = series.drop_duplicates()\n","  #     if gender == male:\n","  #       kwargs = {\"m\" + key:series}\n","  #     else:\n","  #       kwargs = {\"f\" + key:series}\n","  #     df = df.assign(**kwargs)\n","\n","\n","\n","  # path = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/\"\n","  # df.to_csv(path + \"gender_data.csv\")\n","\n","\n","get_descriptors(quotes_file)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ixq3Jfm2UYya"},"source":["character verbs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"elapsed":836,"status":"ok","timestamp":1689807788690,"user":{"displayName":"Arnav Gupta","userId":"02401813659526018071"},"user_tz":420},"id":"LBsc7052UcPP","outputId":"f301e046-3a8e-44fd-8fc8-f7b3192d8b4f"},"outputs":[{"data":{"application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":"download(\"download_2dcf4bbb-07a7-48ee-a3c0-0063dcd062ae\", \"char2.json\", 4997)","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":"\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/javascript":"download(\"download_f0bc8e93-6714-4408-8fe1-897a75318ca0\", \"words2.json\", 1779)","text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"}],"source":["book_file = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/dahl_boy.book\"\n","\n","\n","# def get_NRC_lexicon():\n","#     '''\n","#     @output:\n","#     - A dictionary of format {word : score}\n","#     '''\n","#     lexicon = '/content/drive/MyDrive/Colab_Notebooks/boy_results/NRC-VAD-Lexicon.txt'\n","#     val_dict = {}\n","#     aro_dict = {}\n","#     dom_dict = {}\n","#     with open(lexicon, 'r') as infile:\n","#         reader = csv.DictReader(infile, delimiter=\"\\t\")\n","#         for row in reader:\n","#             word = row['Word']\n","#             val_dict[word] = float(row['Valence'])\n","#             aro_dict[word] = float(row['Arousal'])\n","#             dom_dict[word] = float(row['Dominance'])\n","#     return (val_dict, aro_dict, dom_dict)\n","\n","# # save VAD dictionaries\n","# val_dict, aro_dict, dom_dict = get_NRC_lexicon()\n","\n","import json\n","from google.colab import files\n","\n","def get_adjs(book_file):\n","  with open(book_file, \"r\") as file:\n","    book_data = json.load(file)\n","  char_list = []\n","  words = []\n","  seen = set()\n","  i = 0\n","  for character in book_data[\"characters\"]:\n","    if character[\"mentions\"][\"proper\"]:\n","      name = character[\"mentions\"][\"proper\"][0]['n']\n","    # elif character[\"mentions\"][\"common\"]:\n","    #   name = character[\"mentions\"][\"common\"][0]['n']\n","    else:\n","      continue\n","    char_list.append({\"character\":name, \"words\":[]})\n","    for adj in character[\"mod\"][:10]:\n","      char_list[i][\"words\"].append(adj['w'])\n","      if adj['w'] in seen:\n","        continue\n","      seen.add(adj['w'])\n","      words.append(adj['w'])\n","    for patient in character[\"patient\"][:10]:\n","      char_list[i][\"words\"].append(patient['w'])\n","      if patient['w'] in seen:\n","        continue\n","      seen.add(patient['w'])\n","      words.append(patient['w'])\n","    for agent in character[\"agent\"][:10]:\n","      char_list[i][\"words\"].append(agent['w'])\n","      if agent['w'] in seen:\n","        continue\n","      seen.add(agent['w'])\n","      words.append(agent['w'])\n","      # else:\n","      #   char_list[name][adj['w']] = \"N/A\"\n","    i += 1\n","\n","  # for char in char_list:\n","  #   print(char,\"\\n\")\n","  # for word in words:\n","  #   print(word,\"\\n\")\n","  # for k, v in char_list.items():\n","  #   print(k,v,\"\\n\")\n","  # print(words)\n","\n","  chars_data = json.dumps(char_list)\n","  words_data = json.dumps(words)\n","  path= \"/content/drive/MyDrive/Colab_Notebooks/boy_results\"\n","  chars_file = path + \"/char2.json\"\n","  words_file = path + \"/words2.json\"\n","\n","  with open(chars_file, 'w') as json_file:\n","    json_file.write(chars_data)\n","\n","  with open(words_file, 'w') as json_file:\n","    json_file.write(words_data)\n","\n","  files.download(chars_file)\n","  files.download(words_file)\n","\n","\n","get_adjs(book_file)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wOBK7KhdUeM5"},"source":["#Places"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1393,"status":"ok","timestamp":1689205546763,"user":{"displayName":"Arnav Gupta","userId":"02401813659526018071"},"user_tz":420},"id":"x9VzA9O6Uggk","outputId":"51cec71c-7e35-4a25-9763-fe20fec74739"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import json\n","import pandas as pd\n","import csv\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","entities_file = \"/content/drive/MyDrive/Colab_Notebooks/boy_results/dahl_boy.entities\"\n","\n","def get_places(entities_file):\n","  df = pd.read_csv(entities_file, delimiter = '\\t')\n","  places = []\n","  cat = []\n","\n","  for index, row in df.iterrows():\n","    if row[\"cat\"] != \"PER\" and row[\"prop\"] == \"PROP\":\n","      places.append(row[\"text\"])\n","      cat.append(row[\"cat\"])\n","  result = pd.DataFrame(zip(places, cat))\n","  result = result.drop_duplicates()\n","  result.to_csv(\"/content/drive/MyDrive/Colab_Notebooks/boy_results/places.tsv\")\n","\n","get_places(entities_file)\n","\n"]}],"metadata":{"colab":{"collapsed_sections":["w9VCfGsIOqqo"],"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}
